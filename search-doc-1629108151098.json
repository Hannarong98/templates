[{"title":"React","type":0,"sectionRef":"#","url":"cra/","content":"","keywords":""},{"title":"Create React App","type":1,"pageTitle":"React","url":"cra/#create-react-app","content":"Create React App is the main part of the development environment, these docs only covers the Intility templates for it. This means that everything from the official Create React App docs applies to your project. If you can't find what your looking for here, check the CRA docs. "},{"title":"Recommended reading","type":1,"pageTitle":"React","url":"cra/#recommended-reading","content":"Environment VariablesAdvanced ConfigurationCode Splitting "},{"title":"Deploy with ArgoCD","type":0,"sectionRef":"#","url":"cra/advanced/deploy-argocd","content":"Deploy with ArgoCD info This page isn't finished yet. If you are comfortable with this subject, feel free to contribute.","keywords":""},{"title":"Adding Environments","type":0,"sectionRef":"#","url":"cra/advanced/adding-environments","content":"","keywords":""},{"title":"Enabling the deploy:prod job","type":1,"pageTitle":"Adding Environments","url":"cra/advanced/adding-environments#enabling-the-deployprod-job","content":"The provided deploy:prod job is disabled by default. This is to force you to make an active choice on wether you want to use the job or ArgoCD. To enable the job, simply remove the . from the last job. .gitlab-ci.yml - .deploy:prod:+ deploy:prod: Copy "},{"title":"Trigger production jobs","type":1,"pageTitle":"Adding Environments","url":"cra/advanced/adding-environments#trigger-production-jobs","content":"To trigger the production jobs, simply run npm version [major|minor|patch]git push --tags Copy This will update the version field in package.json, update the appVersion field in chart/Chart.yaml, and create a git tag with the version. After all the other jobs has completed, we can manually trigger the deploy:prod job. This is manual by design, and shouldn't be set to automatic. "},{"title":"Separate OpenShift project","type":1,"pageTitle":"Adding Environments","url":"cra/advanced/adding-environments#separate-openshift-project","content":"You can, and should create a separate OpenShift project to host the production environment. To do this, simply follow the same steps as earlier. This time, name it aa-<project-slug>-prod. When adding the OPENSHIFT_TOKEN to CI/CD variables, select prod as the Environment scope. This variable will then be used instead of the default one in the deploy:prod job.  note If you can't see the prod environment when creating a variable, you need to trigger the production jobs first. The build:prod job will then provision the prod environment, and you can insert the OPENSHIFT_TOKEN before triggering the deploy job. "},{"title":"Adding even more Environments","type":1,"pageTitle":"Adding Environments","url":"cra/advanced/adding-environments#adding-even-more-environments","content":"Usually, a development and a production environment is enough. In big applications however, you might see the need for more environments. In this example, we will set up a QA environment. To do this, we will simply copy the prod jobs, and create our own for qa instead. .gitlab-ci.yml build:qa: extends: build:dev environment: name: qa rules: - if: '$CI_COMMIT_TAG' image:qa: extends: image:dev variables: IMAGE_TAG: qa rules: - if: '$CI_COMMIT_TAG' needs: [ \"build:qa\" ] sentry:qa: extends: sentry:dev variables: SENTRY_ENV: qa rules: - if: '$CI_COMMIT_TAG' needs: [ \"build:qa\" ] .deploy:qa: extends: deploy:dev variables: ROUTE: $CI_PROJECT_NAME-qa.apps.int.intility.no environment: name: qa url: https://$CI_PROJECT_NAME-qa.apps.int.intility.no rules: - if: '$CI_COMMIT_TAG' needs: [ \"test\", \"image:qa\" ] Copy note We don't override the Sentry release and the image version, to avoid version collisions with the production jobs. This example will, like the production jobs, be triggered by git tags. But it will automatically deploy it to the QA environment, which will be created in the default OpenShift project, unless you've created a custom QA project. "},{"title":"Upgrade to V1 pipeline","type":0,"sectionRef":"#","url":"cra/advanced/upgrade-pipeline","content":"","keywords":""},{"title":"Features","type":1,"pageTitle":"Upgrade to V1 pipeline","url":"cra/advanced/upgrade-pipeline#features","content":"Helm chartDocker image pushed to GCRProduction jobsParallelized jobs (pipeline usually takes ~2m) "},{"title":"OpenShift","type":1,"pageTitle":"Upgrade to V1 pipeline","url":"cra/advanced/upgrade-pipeline#openshift","content":"When switching to the new pipeline, it will automatically create new resources. At a minimum, you should delete the existing route, as the old one will crash with the new one. However, it's recommended to start blank, by either clearing out or re-creating the existing project, or by creating a new project. "},{"title":"Create Deploy Token","type":1,"pageTitle":"Upgrade to V1 pipeline","url":"cra/advanced/upgrade-pipeline#create-deploy-token","content":"The new pipeline publishes a docker image to GitLab Container Registry, and uses Helm to create resources in OpenShift. As a result, OpenShift needs access to pull images from GCR. In your GitLab repository, go to Settings -> Repository -> Deploy tokens. Create a new token named gitlab-deploy-token, and give it the read_registry scope.  That's all you have to do in the repository. The token is exposed as a variable in the pipeline, and helm applies it as a pull secret in OpenShift. Read more about GitLab deploy tokens here. "},{"title":"Edit index.ts","type":1,"pageTitle":"Upgrade to V1 pipeline","url":"cra/advanced/upgrade-pipeline#edit-indexts","content":"The new pipeline doesn't use a REACT_APP_REDIRECT_URI environment variable in the pipeline, so we need to change auth.redirectUri in our MSAL Config. index.ts const msal = { auth: { // This is the new value of redirectUri redirectUri: window.location.origin }}; Copy "},{"title":"Copy files","type":1,"pageTitle":"Upgrade to V1 pipeline","url":"cra/advanced/upgrade-pipeline#copy-files","content":"Download the contents of this repository (direct zip download). Copy all files except README.md to your project, and push the changes. The new pipeline should now take effect. Should any problems occur, ask a question in #programming on Slack. "},{"title":"Deploy","type":0,"sectionRef":"#","url":"cra/configuration/deploy","content":"","keywords":""},{"title":"Create project","type":1,"pageTitle":"Deploy","url":"cra/configuration/deploy#create-project","content":"UICLI Go to our OpenShift instance and log in as Intility Developer. Create a project, the name should be aa-<GITLAB_SLUG>-dev, e.g. aa-my-app-dev. Add a fitting display name and description if you feel like it. "},{"title":"Acquire Token","type":1,"pageTitle":"Deploy","url":"cra/configuration/deploy#acquire-token","content":"UICLI In your newly created project, switch from Developer to Administrator view in the sidebar. Then go to User Management -> Service Accounts, and click Create Service Account. Change the name field to gitlab-builder. After creating the Service Account, go to User Mangement -> Role Bindings, and click Create Binding. Fill the form with the following values: Role Binding Name: gitlab-builder-edit Role Name: edit Subject: Service Account Subject Name: gitlab-builder After creating the role binding, go back to Service Accounts and go to the gitlab-builder Service Account. At the bottom of page, you'll find a link to a secret named gitlab-builder-token-*, click it. Copy the token field at the bottom of the page, and add it to GitLab CI/CD variables with the key OPENSHIFT_TOKEN. "},{"title":"Environment Variables","type":0,"sectionRef":"#","url":"cra/configuration/environment-variables","content":"Environment Variables Your project can consume variables declared in your environment as if they were declared locally in your JS files. By default you will have NODE_ENV defined for you, and any other environment variables starting with REACT_APP_. By default, .env.development is included, which will be loaded when you run your app locally. For more information, please read the Environment Variables section of the Create React App docs.","keywords":""},{"title":"GitLab Repository","type":0,"sectionRef":"#","url":"cra/configuration/gitlab","content":"","keywords":""},{"title":"Pushing your application","type":1,"pageTitle":"GitLab Repository","url":"cra/configuration/gitlab#pushing-your-application","content":"Create React App creates a git repository for you locally, but you'll have to add the GitLab repository as a remote. Copy the last three lines of the \"Push an existing Git repository\" example. They should look something like this: git remote add origin git@gitlab.intility.no:Group/intility-app.gitgit push -u origin --allgit push -u origin --tags Copy The pipeline will fail initially, but don't worry, we'll set it up correctly in the next steps. "},{"title":"Add GitLab Deploy Token","type":1,"pageTitle":"GitLab Repository","url":"cra/configuration/gitlab#add-gitlab-deploy-token","content":"The pipeline publishes a docker image to GitLab Container Registry, and uses Helm to create resources in OpenShift. As a result, OpenShift needs access to pull images from GCR. In your GitLab repository, go to Settings -> Repository -> Deploy tokens. Create a new token named gitlab-deploy-token, and give it the read_registry scope.  That's all you have to do in the repository. The token is exposed as a variable in the pipeline, and helm applies it as a pull secret in OpenShift. Read more about GitLab deploy tokens here. "},{"title":"Adding CI/CD Variables","type":1,"pageTitle":"GitLab Repository","url":"cra/configuration/gitlab#adding-cicd-variables","content":"In your project on GitLab, go to Settings -> CI / CD, and expand the Variables section. We don't need to add anything yet, but it's here we'll add tokens and such in the other steps. "},{"title":"Adding Badges","type":1,"pageTitle":"GitLab Repository","url":"cra/configuration/gitlab#adding-badges","content":"In your project on GitLab, go to Settings -> General, and expand the Badges section. Here you can add badges by giving them a Name, Link and Image URL.  We can go ahead and add a badge for our pipeline with the following values: Name: Pipeline Link: https://gitlab.intility.com/%{project_path} Image URL: https://gitlab.intility.com/%{project_path}/badges/%{default_branch}/pipeline.svg "},{"title":".gitlab-ci.yml Overview","type":0,"sectionRef":"#","url":"cra/configuration/gitlab-ci","content":"","keywords":""},{"title":"build","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#build","content":"The main build job runs npm run build on every push to all branches (except master). It will pass some default environment variables; REACT_APP_SENTRY_DSN will be SENTRY_ENVREACT_APP_SENTRY_ENVIRONMENT will be the git branch nameREACT_APP_SENTRY_RELEASE will be the git commit SHAREACT_APP_REDIRECT_URI will be https://{projectSlug}-{branchName}.openshift-inside.intility.no It also creates an artifact of the build folder that is created, which will be passed onto later jobs. The build:* pipeline does the same as build, except it adds a GitLab environment. The base job uses the GitLab environment name to configure the Sentry environment. "},{"title":"test","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#test","content":"This job simply runs the npm test script. "},{"title":"sonarqube","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#sonarqube","content":"This job uses the SonarScanner CLI to upload the source code to SonarQube. The pipeline will continue if this job fails, but it should be looked into. "},{"title":"image","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#image","content":"The image jobs builds a docker image with the Dockerfile.CI dockerfile, and with the build output from the build jobs. For the image:dev job, it pushes two tags, one with dev, and one with the pipeline id. For the image:prod job, the tags pushed are latest and the tag name. "},{"title":"sentry","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#sentry","content":"The sentry job uses the Sentry CLI to create a new release. The release name will be the git commit SHA. It will connect the release to the current commit, and upload sourcemaps from the build step. Lastly it will associate the release with the right env. "},{"title":"deploy","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#deploy","content":"This is the main job for deploying to OpenShift. It will create an environment in GitLab, with the URL to the running app. It uses Helm to install the app to an OpenShift project. For deploy:dev, the route defaults to https://{projectSlug}-dev.apps.int.intility.no. For deploy:prod, the route defaults to https://{projectSlug}.apps.int.intility.no. "},{"title":"Sentry","type":0,"sectionRef":"#","url":"cra/configuration/sentry","content":"","keywords":""},{"title":"Create Project","type":1,"pageTitle":"Sentry","url":"cra/configuration/sentry#create-project","content":"Head over to the Create a new Project page in Sentry.  Under platform, select React.  For the project name, use the project slug from GitLab. Select a fitting team, or create a new one, and hit Create. note If you use something other than the GitLab project slug you'll need to modify the SENTRY_PROJECT variable in the top of the .gitlab-ci.yml file. You'll be taken to a Configure React page. In the code example under Connecting the SDK to Sentry, copy the dsn value, and add it to GitLab CI/CD variables with the key SENTRY_DSN. "},{"title":"Acquire Token","type":1,"pageTitle":"Sentry","url":"cra/configuration/sentry#acquire-token","content":"Go to the Auth Token section of your Account Settings. The needed scopes are project:read, project:releases and org:read. Create it, copy the token and add it to GitLab CI/CD variables with the key SENTRY_AUTH_TOKEN. "},{"title":"SonarQube","type":0,"sectionRef":"#","url":"cra/configuration/sonarqube","content":"","keywords":""},{"title":"Badge","type":1,"pageTitle":"SonarQube","url":"cra/configuration/sonarqube#badge","content":"On your project dashboard, you can click Get project badges in the bottom right corner.  Select Quality Gate Status, and Image URL only. You can then use the Image URL and project dashboard URL to create a badge in GitLab.  "},{"title":"Included Dependencies","type":0,"sectionRef":"#","url":"cra/getting-started/included-dependencies","content":"","keywords":""},{"title":"@intility/bifrost-react","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#intilitybifrost-react","content":"Docs Intility's design system for React. "},{"title":"@intility/react-msal-browser","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#intilityreact-msal-browser","content":"Docs (WIP) @intility/react-* is an abstraction of the 3 auth libraries (adal, msal, msal-browser) by Microsoft for React. "},{"title":"react-router","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#react-router","content":"Docs (v6) Unless you're a dashboard, you need a router, and react-router is the de facto router for React. "},{"title":"@sentry/react and @sentry/tracing","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#sentryreact-and-sentrytracing","content":"Docs Used for error reporting to Sentry. "},{"title":"prettier","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#prettier","content":"Docs Prettier is a code formatter and great tool for unifying formatting when collaborating on a project. The Visual Studio Code Workspace is set up to auto-format on save. "},{"title":"source-map-explorer","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#source-map-explorer","content":"GitHub See Analyzing the Bundle Size for more information. You can use it by running the following command in your project. npm run analyze Copy "},{"title":"Installation","type":0,"sectionRef":"#","url":"cra/getting-started/installation","content":"","keywords":""},{"title":"Prerequisites","type":1,"pageTitle":"Installation","url":"cra/getting-started/installation#prerequisites","content":"Some experience with Reactnode.jsVisual Studio Code with the following plugins Node Extension PackPrettier Set up the Intility NPM registry (optional) "},{"title":"Intility NPM Registry (optional)","type":1,"pageTitle":"Installation","url":"cra/getting-started/installation#intility-npm-registry-optional","content":"The Intility NPM registry (includes access to @intility/* and @fortawesome/* packages) can be configured by running: npm config set registry https://artifactory.int.intility.no/artifactory/api/npm/npm/ Copy For other alternatives, see the guide on developers.intility.no (login required). "},{"title":"Create App","type":1,"pageTitle":"Installation","url":"cra/getting-started/installation#create-app","content":"Select the version you want. By default, TypeScript and an authentication sample is included. default git branch create-react-app automatically creates a git repo. If you want the default branch to be something other than master, you need to set the init.defaultBranch config in git: git config --global init.defaultBranch main You need git version 2.28.0 or higher to use this option. TypeScriptJavaScript npx create-react-app my-app --template \"@intility\" Copy Run it in your terminal, and then open the Visual Studio Code Workspace with the following command. code my-app/app.code-workspace Copy You can now start coding by running the start script. npm start Copy "},{"title":"Available Scripts","type":1,"pageTitle":"Installation","url":"cra/getting-started/installation#available-scripts","content":"There are 5 scripts included by default, click on each one for more information. npm startnpm testnpm run buildnpm run ejectnpm run analyze "},{"title":"Project Overview","type":0,"sectionRef":"#","url":"cra/getting-started/project-overview","content":"Project Overview Depending on which template you chose, your project structure will look more or less like the following: ├── public│ ├── favicon.ico│ ├── index.html│ ├── logo192.png│ ├── logo512.png│ ├── manifest.json│ └── robots.txt├── src│ ├── assets│ │ └── logo.svg│ ├── components│ │ ├── About.tsx│ │ ├── App.css│ │ ├── App.test.tsx│ │ ├── App.tsx│ │ ├── Home.tsx│ │ └── Profile.tsx│ ├── index.tsx│ ├── react-app-env.d.ts│ ├── serviceWorker.ts│ └── setupTests.ts├── .env.development├── .eslintrc├── .gitignore├── .gitlab-ci.yml├── .npmrc├── Dockerfile├── README.md├── app.code-workspace├── package-lock.json├── package.json└── tsconfig.json Copy","keywords":""},{"title":"Recommended Packages","type":0,"sectionRef":"#","url":"cra/getting-started/recommended-packages","content":"","keywords":""},{"title":"react-table","type":1,"pageTitle":"Recommended Packages","url":"cra/getting-started/recommended-packages#react-table","content":"Docs Data modification for tables. Bifrost's Table component is only responsible for the view, and will not do any data modification for you. "},{"title":"swr","type":1,"pageTitle":"Recommended Packages","url":"cra/getting-started/recommended-packages#swr","content":"Docs Data Fetching library using React hooks. Makes data fetching easy, comes with a cache, Suspense support and more. "},{"title":"AAD Authentication","type":0,"sectionRef":"#","url":"cra/configuration/authentication","content":"","keywords":""},{"title":"UI","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#ui","content":"Head over to Azure -> Azure Active Directory -> App registrationswith your Intility Account, and create a new registration.  Select a fitting name for your project, this name will be presented to the user during consent. Under Supported account types, choose either Intility AS only - Single tenant or Any Azure AD directory - Multitenant. This can be changed later, so if you're unsure what to choose, select single tenant. Under Redirect URI, select Single-page application (SPA) from the dropdown, and enter http://localhost:3000. Hit the register button, and you will be taken to an overview of your newly created registration.  Copy the Application (Client) ID GUID, and paste it into the clientId field in your index.tsx file. src/index.tsx const msal = { auth: { clientId: \"YOUR_CLIENT_ID\", ... }, ...} Copy Check out Enabling authentication in your app to enable authentication in your app. "},{"title":"Adding reply URLs","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#adding-reply-urls","content":"For each deployment of your app, you'll need to register it. You can do that by going to the Authentication page.  The first we need to add is the URL the deploy step makes in OpenShift: https://{your-project-slug}-dev.apps.int.intility.no Copy You can also add more later if you create more environments. "},{"title":"CLI","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#cli","content":"Login with the Azure CLI and run the following command (rember to modify the displayName) bashPowerShell az rest -u https://graph.microsoft.com/v1.0/applications -m post -b '{\"displayName\":\"MyApp\",\"spa\":{\"redirectUris\":[\"http://localhost:3000\"]}}' Copy Copy the appId GUID, and paste it into the clientId field in your index.tsx file. src/index.tsx const msal = { auth: { clientId: \"YOUR_CLIENT_ID\", ... }, ...} Copy Check out Enabling authentication in your app to enable authentication in your app. "},{"title":"Enabling authentication in your app","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#enabling-authentication-in-your-app","content":"Depending on what your applications scope is, there are different ways of enabling authentication. "},{"title":"Forcing auth","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#forcing-auth","content":"This is the simplest way, and can be enabled by adding the forced prop to MsalBrowserProvider. src/index.tsx ReactDOM.render( <Router> <MsalBrowserProvider config={msal} forced> ^^^^^^ <React.StrictMode> <App /> </React.StrictMode> </MsalBrowserProvider> </Router>, document.getElementById(\"root\")) Copy This will force user authentication, and your app won't render unless the user is authenticated. "},{"title":"Conditional auth","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#conditional-auth","content":"If you don't want forced auth, you can implement a simple log-in button by using the useAuth hook from @intility/react-msal-browser. import { Button } from \"@intility/bifrost-react\";import { useAuth } from \"@intility/react-msal-browser\"; const LoginButton = () => { const { login } = useAuth(); return ( <Button onClick={login}>Login</Button> )}; Copy And elsewhere in your app you can determin the login status by checking the existance of a user with useUser. import { useUser } from \"@intility/react-msal-browser\"; const MyComponent = () => { const user = useUser(); return <span>{user !== null ? \"Hello friend.\" : \"Who are you?\"}</span>;}; Copy "},{"title":"Consuming an authenticated API","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#consuming-an-authenticated-api","content":"Consuming an authenticated API is relativly simple. First, you'll need to modify the endpoints section of your MSAL config. The key needs to be the base URL of the API, and the value should be an array of scopes needed to authenticate to the API. src/index.tsx const msal = { endpoints: { \"http://localhost:5000\": [\"API_SCOPE\"], ... }, ...} Copy The API base URL can also be dynamically applied from an Environment Variables. TypeScriptJavaScript src/index.tsx const msal = { endpoints: { [process.env.REACT_APP_API_URL as string]: [\"API_SCOPE\"], ... }, ...} Copy You can now consume an API with authorizedFetch, which uses the endpoints config to detect which token to use in a request. import { useState, useEffect } from \"react\";import { authorizedFetch } from \"@intility/react-msal-browser\"; // This is a simplified example of data fetching in react// Please don't use in production const DataComponent = () => { const [data, setData] = useState(); const [error, setError] = useState(); useEffect(() => { authorizedFetch(\"my-api/resource\") .then((response) => response.json()) .then((json) => setData(json)) .catch((error) => setError(error)); }, []); return ( <> {!data && !error && <p>Loading data...</p>} {data && <div>Here is the data: {data}</div>} {error && <p className=\"error\">Oh No!!! {error.toString()}</p>} </> );}; Copy "},{"title":"SWR / React Query","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#swr--react-query","content":"SWRReact Query Integrating authenticatedFetch with SWR is very simple. All you need to do is specify a fetcher for SWR. TypeScriptJavaScript import useSWR from \"swr\";import { authorizedFetch } from \"@intility/react-msal-browser\"; const authFetcher = (url: string) => authorizedFetch(url).then((result) => result.json()); const SwrComponent = () => { const { data, error } = useSWR(\"my-api/resource\", { fetcher: authFetcher }); return ( <> {!data && !error && <p>Loading data...</p>} {data && <div>Here is the data: {data}</div>} {error && <p className=\"error\">Oh No!!! {error.toString()}</p>} </> );}; Copy We could take this one step further, and assign the fetcher globally using SWRConfig. TypeScriptJavaScript src/index.tsx import { SWRConfig } from \"swr\";import { authorizedFetch } from \"@intility/react-msal-browser\"; // Note that we apply a base url to every requestconst swr = { fetcher: (path: string) => authorizedFetch((process.env.REACT_APP_API_URL as string) + path) .then(response => response.json())}; ReactDOM.render( <Router> <MsalBrowserProvider config={msal} forced> <SWRConfig value={swr}> <React.StrictMode> <App /> </React.StrictMode> </SWRConfig> </MsalBrowserProvider> </Router>, document.getElementById(\"root\")); Copy We can now call useSWR somewhere in our app, and it will by default use the authorized fetcher. import useSWR from \"swr\";import { authorizedFetch } from \"@intility/react-msal-browser\"; const SwrComponent = () => { // Note that only the resource is requested here // that's because the API base URL is prepended in our fetcher const { data, error } = useSWR(\"resource\"); return ( <> {!data && !error && <p>Loading data...</p>} {data && <div>Here is the data: {data}</div>} {error && <p className=\"error\">Oh No!!! {error.toString()}</p>} </> );}; Copy One great thing about SWR is request deduping and caching. In the following example, there will be three instances of our SwrComponent, but only one request will be made. That would not be the case if we were to use the DataComponent from earlier, where we manually fetched the data. const SomeExample = () => ( <> <SwrComponent /> <SwrComponent /> <SwrComponent /> </>); Copy To read more about SWR, check out the SWR Docs. "},{"title":".NET Templates","type":0,"sectionRef":"#","url":"dotnet/","content":"","keywords":""},{"title":"Recommended reading","type":1,"pageTitle":".NET Templates","url":"dotnet/#recommended-reading","content":"Tutorial: Create a web API with ASP.NET CoreASP.NET Core fundamentalsIntility Template Source "},{"title":"Azure App Config","type":0,"sectionRef":"#","url":"dotnet/advanced/azure-app-config","content":"","keywords":""},{"title":"Create App Configuration","type":1,"pageTitle":"Azure App Config","url":"dotnet/advanced/azure-app-config#create-app-configuration","content":"In Azure, create an App Configuration, and go to it.  Copy the endpoint URL, and paste it into the AppConfig field in Properties/launchSettings.json. Properties/launchSettings.json { \"profiles\": { \"YOUR_PROJECT_NAME\": { ... \"environmentVariables\": { \"AppConfig\": \"MY_APP_CONFIG_ENDPOINT\" } } ... }} Copy The template is set up to use built-in credentials of your development machine. This means that we need to allow your (or more) accounts to access the App Configuration. Go to Access Control (IAM) in the sidebar, then click Add and Add role assignment. Select App Configuration Data Owner, and your own account or a group.  info If you do not have access to add role assignments, you should as someone with the role assignment Owner to do it for you. You can now add Key-values in Configuration manager, and they will be applied to your local development environment. "},{"title":"Create Key Vault","type":1,"pageTitle":"Azure App Config","url":"dotnet/advanced/azure-app-config#create-key-vault","content":"In Azure, create a Key Vault. When setting up access policies, allow the same group/users you set up in your App Configuration to Get and List the Keys and Secrets. You can now add Key Vault references in the App Configurations Configuration manager, and they will be applied to your local development environment. "},{"title":"Use in development deployment","type":1,"pageTitle":"Azure App Config","url":"dotnet/advanced/azure-app-config#use-in-development-deployment","content":"Since the config uses your machines credentials to access the App Configuration and Key Vault, it won't automatically work with your development deployment. To set this up, we need to grant your App Registration we set up earlier access to the App Configuration and Key Vault. First, give it the App Configuration Data Reader role in the App Configuration. Then, give it Get and List permissions for Key and Secrets. You also need a client secret for your App Registration, create one in Azure AD -> App Registrations -> Your App Registration -> Certificates & secrets. Set up the following variables in GitLab CI/CD under the Development environment scope AppConfig: The App Configuration endpointAZURE_CLIENT_ID: The App Registration client idAZURE_CLIENT_SECRET: The client secret you just madeAZURE_TENANT_ID: The tenant id of your App Registration (check in your App Registrations Overview) Lastly, we need to configure the development deployment to use these variables. We do this by passing the variables we just set up to Helm: .gitlab-ci.yml deploy:dev: ... script: - oc login $OPENSHIFT_SERVER --token=$OPENSHIFT_TOKEN - helm upgrade --install $CI_ENVIRONMENT_SLUG ./Company.WebApplication1/chart --set nameOverride=$CI_PROJECT_NAME --set image.repository=$CI_REGISTRY_IMAGE --set image.tag=$IMAGE_TAG --set registry.url=$CI_REGISTRY --set registry.user=$CI_DEPLOY_USER --set registry.password=$CI_DEPLOY_PASSWORD --set route=$ROUTE --set replicaCount=2 --set secrets.Sentry__Dsn=$SENTRY_DSN --set config.ASPNETCORE_ENVIRONMENT=$CI_ENVIRONMENT_NAME --set config.AppConfig=$AppConfig --set config.AZURE_CLIENT_ID=$AZURE_CLIENT_ID --set config.AZURE_CLIENT_SECRET=$AZURE_CLIENT_SECRET --set config.AZURE_TENANT_ID=$AZURE_TENANT_ID Copy The development environment will now be able to connect and use the configuration from the App Registration and Key vault. note Don't worry, even though the deploy:prod job extends the deploy:dev job, the variables won't be set, since we only scoped them to the Development environment. "},{"title":"Adding Environments","type":0,"sectionRef":"#","url":"dotnet/advanced/adding-environments","content":"","keywords":""},{"title":"Enabling the deploy:prod job","type":1,"pageTitle":"Adding Environments","url":"dotnet/advanced/adding-environments#enabling-the-deployprod-job","content":"The provided deploy:prod job is disabled by default. This is to force you to make an active choice on wether you want to use the job or ArgoCD. To enable the job, simply remove the . from the last job. .gitlab-ci.yml - .deploy:prod:+ deploy:prod: Copy "},{"title":"Trigger production jobs","type":1,"pageTitle":"Adding Environments","url":"dotnet/advanced/adding-environments#trigger-production-jobs","content":"First, we have to update the appVersion field in our Helm charts Chart.yaml. chart/Chart.yaml appVersion: \"x.x.x\" Copy Then, we have to create a git tag and push it git tag x.x.xgit push --tags Copy After all the other jobs has completed, we can manually trigger the deploy:prod job. This is manual by design, and shouldn't be set to automatic. "},{"title":"Separate OpenShift project","type":1,"pageTitle":"Adding Environments","url":"dotnet/advanced/adding-environments#separate-openshift-project","content":"You can, and should create a separate OpenShift project to host the production environment. To do this, simply follow the same steps as earlier. This time, name it aa-<project-slug>-prod. When adding the OPENSHIFT_TOKEN to CI/CD variables, select Production as the Environment scope. This variable will then be used instead of the default one in the deploy:prod job.  note If you can't see the Production environment when creating a variable, you need to trigger the production jobs first. The image:prod job will then provision the Production environment, and you can insert the OPENSHIFT_TOKEN before triggering the deploy job. "},{"title":"Adding even more Environments","type":1,"pageTitle":"Adding Environments","url":"dotnet/advanced/adding-environments#adding-even-more-environments","content":"Usually, a development and a production environment is enough. In big applications however, you might see the need for more environments. In this example, we will set up a QA environment. To do this, we will simply copy the prod jobs, and create our own for qa instead. .gitlab-ci.yml image:qa: extends: image:dev variables: IMAGE_TAG: qa environment: name: QA rules: - if: '$CI_COMMIT_TAG' .deploy:qa: extends: deploy:dev variables: ROUTE: $CI_PROJECT_NAME-qa.apps.int.intility.no environment: name: QA url: https://$CI_PROJECT_NAME-qa.apps.int.intility.no rules: - if: '$CI_COMMIT_TAG' needs: [ \"test\", \"image:qa\" ] Copy note We don't override the image version to avoid version collisions with the production jobs. This example will, like the production jobs, be triggered by git tags. But it will automatically deploy it to the QA environment, which will be created in the default OpenShift project, unless you've created a custom QA project. "},{"title":"Deploy with ArgoCD","type":0,"sectionRef":"#","url":"dotnet/advanced/deploy-argocd","content":"Deploy with ArgoCD info This page isn't finished yet. If you are comfortable with this subject, feel free to contribute.","keywords":""},{"title":"Installation","type":0,"sectionRef":"#","url":"dotnet/installation","content":"","keywords":""},{"title":"Bootstrap new project using the template","type":1,"pageTitle":"Installation","url":"dotnet/installation#bootstrap-new-project-using-the-template","content":"You can either use the dotnet new command or Visual Studio -> New Project wizard to create a new project based on the templates. dotnet CLIVisual Studio # create projectdotnet new iwebapi -o MyService # run projectcd MyServicedotnet run --project MyService/MyService.csproj Copy "},{"title":"Updating the template","type":1,"pageTitle":"Installation","url":"dotnet/installation#updating-the-template","content":"warning Earlier versions of the CLI had troubles updating packages due to long-lived caches. You need to be on version 5.0.301 or higher for updating to work properly. note This will not update already bootstrapped projects. Check for updates by running dotnet new --update-check Copy If there are any updates available, update with dotnet new --update-apply Copy or dotnet new --install Intility.Templates Copy "},{"title":"Features","type":0,"sectionRef":"#","url":"dotnet/features","content":"Features The template is packed with already made decisions, so you don't have to make them. These solutions are a culmination of many years of managing services in production and aims to target established in-house infrastructure. note The templates are updated as the technology and infrastructure changes over time. Helm chartProject debug settingsSwagger configurationDockerfilesREADME templateGitLab CI/CD pipelineAPI VersioningLogging configurationTelemetry instrumentation","keywords":""},{"title":"Deploy","type":0,"sectionRef":"#","url":"dotnet/setup/deploy","content":"","keywords":""},{"title":"Create project","type":1,"pageTitle":"Deploy","url":"dotnet/setup/deploy#create-project","content":"UICLI Go to our OpenShift instance and log in as Intility Developer. Create a project, the name should be aa-<GITLAB_SLUG>-dev, e.g. aa-my-api-dev. Add a fitting display name and description if you feel like it. "},{"title":"Acquire Token","type":1,"pageTitle":"Deploy","url":"dotnet/setup/deploy#acquire-token","content":"UICLI In your newly created project, switch from Developer to Administrator view in the sidebar. Then go to User Management -> Service Accounts, and click Create Service Account. Change the name field to gitlab-builder. After creating the Service Account, go to User Mangement -> Role Bindings, and click Create Binding. Fill the form with the following values: Role Binding Name: gitlab-builder-edit Role Name: edit Subject: Service Account Subject Name: gitlab-builder After creating the role binding, go back to Service Accounts and go to the gitlab-builder Service Account. At the bottom of page, you'll find a link to a secret named gitlab-builder-token-*, click it. Copy the token field at the bottom of the page, and add it to GitLab CI/CD variables with the key OPENSHIFT_TOKEN. "},{"title":"AAD Authorization","type":0,"sectionRef":"#","url":"dotnet/setup/authorization","content":"","keywords":""},{"title":"API","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#api","content":"Head over to Azure -> Azure Active Directory -> App registrationswith your Intility Account, and create a new registration.  Select a fitting name for your project, this name will be presented to the user during consent. Under Supported account types, choose either Intility AS only - Single tenant or Any Azure AD directory - Multitenant. This can be changed later, so if you're unsure what to choose, select single tenant. Under Redirect URI, select Web from the dropdown, and enter http://localhost:5000. Hit the register button, and you will be taken to an overview of your newly created registration.  Copy the Application (Client) ID GUID, and paste it into the AzureAd:ClientId field in your appsettings.json file. appsettings.json { \"AzureAd\": { \"ClientId\": \"YOUR_CLIENT_ID\", ... }, ...} Copy info If you chose Multitenant, set the AzureAd:TenantId field to common. "},{"title":"Add an application scope","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#add-an-application-scope","content":"Go to Expose an API in your app registration, and add a scope. You will be prompted to set an Application ID URI. Use the suggested one, and hit Save and continue.  Add a scope named user_impersonation, that can be consented by Admins and users. You can use the following descriptions: Access API as userAllows the app to access the API as the user. Access API as youAllows the app to acces the API as you. Copy "},{"title":"Swagger","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#swagger","content":"In addition to creating an App Registration for the API itself, we need to make one for the Swagger client too. Again head over to Azure -> Azure Active Directory -> App registrations.  Use the same name appended with Swagger. Under Redirect URI, select Single-page application (SPA) and enter http://localhost:5000/oauth2-redirect.html. Hit the register button, and you will be taken to an overview of your newly created registration.  Copy the Application (Client) ID GUID, and paste it into the AzureAd:ClientId field in your appsettings.json file. appsettings.json { \"Swagger\": { \"ClientId\": \"YOUR_SWAGGER_CLIENT_ID\", ... }, ...} Copy "},{"title":"Adding reply URLs","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#adding-reply-urls","content":"For each deployment of your app, you'll need to register it. You can do that by going to the Authentication page.  The first reply URLs we need to add are the localhost https URL, and the OpenShift deploy URL: https://localhost:5001/oauth2-redirect.htmlhttps://{your-project-slug}-dev.apps.int.intility.no/oauth2-redirect.html Copy You can also add more later if you create more environments. "},{"title":"Access API","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#access-api","content":"To allow Swagger to talk to the API, you need to add API permissions to the Swagger app registration. Go to API permissions, and hit Add a permission. Under My APIs, find your API, select the scope(s) and press Add permissions.  "},{"title":"Guest users","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#guest-users","content":"The template comes with an authorization policy that denies guest users in Azure AD from accessing the API. This policy is enabled when the application is not set up as multitenant. If you want guest users to access your single tenant API, simply remove the lines applying the policy. Startup.cs services.AddAuthorization(options =>{ var tenantId = Configuration[\"AzureAd:TenantId\"]; if (tenantId != \"common\" && tenantId != \"organizations\") { options.AddPolicy(\"NoGuests\", policy => policy.RequireClaim( ClaimConstants.TenantId, tenantId)); }}); Copy "},{"title":"GitLab Repository","type":0,"sectionRef":"#","url":"dotnet/setup/gitlab","content":"","keywords":""},{"title":"Pushing your application","type":1,"pageTitle":"GitLab Repository","url":"dotnet/setup/gitlab#pushing-your-application","content":"Create React App creates a git repository for you locally, but you'll have to add the GitLab repository as a remote. Follow the \"Push an existing folder\" example. They should look something like this: git initgit remote add origin git@gitlab.intility.no:Group/intility-api.gitgit add .git commit -m \"Initial commit\"git push -u origin master Copy The pipeline will fail initially, but don't worry, we'll set it up correctly in the next steps. "},{"title":"Add GitLab Deploy Token","type":1,"pageTitle":"GitLab Repository","url":"dotnet/setup/gitlab#add-gitlab-deploy-token","content":"The pipeline publishes a docker image to GitLab Container Registry, and uses Helm to create resources in OpenShift. As a result, OpenShift needs access to pull images from GCR. In your GitLab repository, go to Settings -> Repository -> Deploy tokens. Create a new token named gitlab-deploy-token, and give it the read_registry scope.  That's all you have to do in the repository. The token is exposed as a variable in the pipeline, and helm applies it as a pull secret in OpenShift. Read more about GitLab deploy tokens here. "},{"title":"Adding CI/CD Variables","type":1,"pageTitle":"GitLab Repository","url":"dotnet/setup/gitlab#adding-cicd-variables","content":"In your project on GitLab, go to Settings -> CI / CD, and expand the Variables section. We don't need to add anything yet, but it's here we'll add tokens and such in the other steps. "},{"title":"Adding Badges","type":1,"pageTitle":"GitLab Repository","url":"dotnet/setup/gitlab#adding-badges","content":"In your project on GitLab, go to Settings -> General, and expand the Badges section. Here you can add badges by giving them a Name, Link and Image URL.  We can go ahead and add a badge for our pipeline with the following values: Name: Pipeline Link: https://gitlab.intility.com/%{project_path} Image URL: https://gitlab.intility.com/%{project_path}/badges/%{default_branch}/pipeline.svg "},{"title":".gitlab-ci.yml Overview","type":0,"sectionRef":"#","url":"dotnet/setup/gitlab-ci","content":"","keywords":""},{"title":"Jobs","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#jobs","content":"info The image:dev and image:prod jobs is set to run on pushes to master or main branch. info The image:prod and deploy:prod jobs are set to run when tags are pushed to the repository. "},{"title":"build","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#build","content":"Builds the project, and creates a build artifact for later stages to use. "},{"title":"test","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#test","content":"Runs the tests in the project. Runs parallel with build. "},{"title":"analyze","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#analyze","content":"Analyzes the project with sonar-scanner. Runs parallel with build. "},{"title":"image","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#image","content":"Uses the artifact from the build step, and creates and publish a docker image with the Dockerfile.CI file using kaniko. Runs once the build job has finished. "},{"title":"deploy","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#deploy","content":"Deploys the application to OpenShift using the Helm chart. Runs once the image and test jobs have succeeded. "},{"title":"Debugging the pipeline","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#debugging-the-pipeline","content":""},{"title":"Paths","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#paths","content":"Ensure the paths used are right. In the build step, we need to ensure that we are working from the right folder. .gitlab-ci.yml build: script: - dotnet restore - cd YOUR_PROJECT_FOLDER # cd to the project folder - dotnet publish -o build artifacts: paths: - YOUR_PROJECT_FOLDER/build # the build in the project folder Copy In image:dev, we need the context parameter to be set to the project folder. .gitlab-ci.yml image:dev: script: - echo ... - /kaniko/executor --context $CI_PROJECT_DIR/YOUR_PROJECT_FOLDER # sets the context to your project folder Copy In deploy:dev, ensure the path to the chart is right. .gitlab-ci.yml deploy:dev: script: - oc login $OPENSHIFT_SERVER --token=$OPENSHIFT_TOKEN - helm upgrade --install $CI_ENVIRONMENT_SLUG ./YOUR_PROJECT_FOLDER/chart # path to chart Copy "},{"title":"SonarQube","type":0,"sectionRef":"#","url":"dotnet/setup/sonarqube","content":"","keywords":""},{"title":"Badge","type":1,"pageTitle":"SonarQube","url":"dotnet/setup/sonarqube#badge","content":"On your project dashboard, you can click Get project badges in the bottom right corner.  Select Quality Gate Status, and Image URL only. You can then use the Image URL and project dashboard URL to create a badge in GitLab.  "},{"title":"Sentry","type":0,"sectionRef":"#","url":"dotnet/setup/sentry","content":"","keywords":""},{"title":"Create Project","type":1,"pageTitle":"Sentry","url":"dotnet/setup/sentry#create-project","content":"Head over to the Create a new Project page in Sentry.  Under platform, select .NET.  For the project name, use the project slug from GitLab. Select a fitting team, or create a new one, and hit Create. You'll be taken to a Configure .NET page. In the code examples, copy the DSN value passed to Sentry.Init, and add it to GitLab CI/CD variables with the key SENTRY_DSN. "},{"title":"Configuration","type":0,"sectionRef":"#","url":"dotnet/topics/configuration","content":"","keywords":""},{"title":"Azure AD Authentication","type":1,"pageTitle":"Configuration","url":"dotnet/topics/configuration#azure-ad-authentication","content":" ⚠️ TODO: App RegistrationsThe relationship between SPA App, Resource App How to configure them properly  { \"AzureAd\": { // azure identity platform instance (should not be changed) \"Instance\": \"https://login.microsoftonline.com/\", // primary domain of your tenant \"Domain\": \"intility.onmicrosoft.com\", // tenant / directory id \"TenantId\": \"9b5ff18e-53c0-45a2-8bc2-9c0c8f60b2c6\", // client id of your azure appReg \"ClientId\": \"11111111-1111-1111-11111111111111111\" }} Copy "},{"title":"Swagger","type":1,"pageTitle":"Configuration","url":"dotnet/topics/configuration#swagger","content":" ⚠️ TODO: Swagger authenticationSwagger versioning relationship with API versioning  { \"Swagger\": { // Name of the service in Swagger definition and UI \"AppName\": \"Company.WebApplication1 Swagger\", // The client id to authenticate with. This should be an // app registration with delegated permission to the API registration \"ClientId\": \"22222222-2222-2222-22222222222222222\" }} Copy "},{"title":"Logging","type":1,"pageTitle":"Configuration","url":"dotnet/topics/configuration#logging","content":"Logging is provided by the external package Intility.Logging.AspNetCore in conjunction with logger sink extensions. The goal with the external package is to enable continuous development of logging support centrally, as the infrastructure changes over time, alleviating you of this burden. note For more information about logging configurations visit the Logging section. { \"Serilog\": { \"MinimumLevel\": { \"Default\": \"Information\", \"Override\": { \"System\": \"Warning\", \"Microsoft\": \"Warning\", \"Microsoft.Hosting.Lifetime\": \"Information\" } }, \"Properties\": { \"Application\": \"MyService\" } }} Copy "},{"title":"Express TypeScript","type":0,"sectionRef":"#","url":"express/","content":"","keywords":""},{"title":"Questions and Contributions","type":1,"pageTitle":"Express TypeScript","url":"express/#questions-and-contributions","content":"While this setup might work, it is nor perfect or complete! Maybe I'm missing some key part? or you have a suggestion on something that can be done different or in another way? If you have any questions or suggestions, please send an merge request on the GitLab project located HERE or message me at Microsoft Teams "},{"title":"Goals","type":1,"pageTitle":"Express TypeScript","url":"express/#goals","content":"Provide an ready to develop TypeScript configuration setup.Ready configured EsLint with the most common rules.Async error handling.Logging to Azure Application Insights.Integrate common security measures: CORSAuthentication and authorization using Passport as well as custom common token rules. Including Role-based access control, guest user validation and checking user tenant. "},{"title":"Getting started using this template","type":1,"pageTitle":"Express TypeScript","url":"express/#getting-started-using-this-template","content":""},{"title":"Option 1: Cloning","type":1,"pageTitle":"Express TypeScript","url":"express/#option-1-cloning","content":"Just clone this repo and point to your new origin! Run git clone git@gitlab.intility.com:digital-telco/utils/NodeTypeScriptProjectTemplate.git --depth 1Create your own GitLab project here: Create blank projectRun git remote set-url origin <YOUR_NEW_PROJECT>Navigate into the directory and run npm install (or yarn install if you prefer yarn) "},{"title":"Option 2: Forking","type":1,"pageTitle":"Express TypeScript","url":"express/#option-2-forking","content":"Head to https://gitlab.intility.com/digital-telco/utils/NodeTypeScriptProjectTemplateHit the Fork-button in the upper right part of the screen.Select where you want to save the fork.Clone your fork.Navigate into the directory and run npm install (or yarn install if you prefer yarn)If you want to remove the fork relationship to this repo. Go to: Settings -> General -> Advanced -> Remove fork relationship "},{"title":"Logging","type":0,"sectionRef":"#","url":"dotnet/topics/logging","content":"","keywords":""},{"title":"Creating Logs","type":1,"pageTitle":"Logging","url":"dotnet/topics/logging#creating-logs","content":"To create logs, use an ILogger<TCategoryName> object from dependency injection (DI). Let's take a look at the following example: [ApiController][Route(\"[controller]\")]public class CustomerController : ControllerBase{ private readonly ILogger _logger; public CustomerController(ILogger<CustomerController> logger) { _logger = logger; } [HttpPost] public ActionResult<Customer> Create(CreateCustomerDTO createCustomerDto) { _logger.LogInformation( \"Customer endpoint called at {Timestamp}\", DateTime.UtcNow.ToLongTimeString()); //... }} Copy The DI system creates a logger and injects it into the controller. In this example the logger uses a log category of the fully qualified name of the type CustomerController. The log category is a string that is associated with each log entry and makes it easier to troubleshoot large quantities of logs. One can also observe that we are logging with the LogInformation extension method. Information is one of several log levels available - ordered by criticality: Debug:General debug information used in development environments with high verbosity Trace: Events related to code performance and infrastructure Information:Low criticality application information Warning:Medium criticality information and recoverable exceptions Error:Hight criticality information about non-recoverable exceptions and faulty states Critical:Severely critical messages related to crashes where the process cannot recover "},{"title":"Structured Logging","type":1,"pageTitle":"Logging","url":"dotnet/topics/logging#structured-logging","content":"In addition to the message we are also taking advantage of the structured logging capabilities baked into the logger. This means that we can inject arbitrary metadata surrounding our log events to make troubleshooting even easier. In the example above we are injecting the current timestamp into a property called Timestamp. Further more, we can also pass inn serializable objects like reference types or records. Serilog will automatically serialize the properties for you. Note that you have to prefix your property name with an @ symbol for this serialization to take place var albert = new { Name = \"Albert Aaberg\" };logger.LogWarning(\"Could not find any friends of {@User}\", albert); Copy "},{"title":"Scoping","type":1,"pageTitle":"Logging","url":"dotnet/topics/logging#scoping","content":"It is sometimes useful to attach common metadata to a series of log events in your application instead of placing them separately in the log message itself. var props = new Dictionary<string,object>(){ { \"Name\", \"Albert Aaberg\"}, { \"Friends\", 0 }}; using(logger.BeginScope(props)){ logger.LogInformation(\"Every log message inside this scope has metadata attached\");} logger.LogInformation(\"The props has left the building\"); Copy  "},{"title":"Configure Logging","type":1,"pageTitle":"Logging","url":"dotnet/topics/logging#configure-logging","content":"With the serilog configuration it is possible to override the verbosity of the logs for each namespace separately. Very useful if you need to up the verbosity of your own code without clutter. NB: The overrides should always be stricter than the default MinimumLevel. There is also a very handy Properties section that lets you define custom metadata surrounding all your log events. The application host can also inject additional context to your logs through environment variables through the same configuration infrastructure. Configuration is usually provided by the Serilog section of the appsettings.{Environment}.json files. The following appsettings.json file is generated by the template. { \"Serilog\": { \"MinimumLevel\": { \"Default\": \"Information\", \"Override\": { \"System\": \"Warning\", \"Microsoft\": \"Warning\", \"Microsoft.Hosting.Lifetime\": \"Error\" } }, \"Properties\": { \"Application\": \"MyService\" }} Copy You can read more about logging at the official .NET Core documentation and by visiting the Serilog documentation for a complete overview of the configuration capabilities. Logging in .NET Core and ASP.NET Core~55 minutes to read "},{"title":"Azure - Event Grid","type":0,"sectionRef":"#","url":"express/API/Azure/Event-Grid","content":"Azure - Event Grid In this chapter i'll go publishing events to Intility's EventGrid called Metro. Guides to what Metro is and how is works can be found here: Official Intility documentation on Metro TODO write text and create examples JS sample on publishing eventsJS sample on subscribing to events","keywords":""},{"title":"Azure - Application Insights","type":0,"sectionRef":"#","url":"express/API/Azure/Application-Insights","content":"Azure - Application Insights NOTE: This assumes that you have an Azure Application Insights resource created for your project applicationinsights - Microsoft Application Insights module for Node.js. npm i applicationinsights Find your instrumentation key in Azure Portal here: And add this to a new secret in your .env file: APPINSIGHTS_INSTRUMENTATIONKEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx Copy Next create a new file called appInsights.ts in your config folder. import * as appInsights from 'applicationinsights'; const { NODE_ENV, APPINSIGHTS_INSTRUMENTATIONKEY } = process.env; if (!NODE_ENV) { throw new Error(\"Missing Environment variable: NODE_ENV\")} if (!APPINSIGHTS_INSTRUMENTATIONKEY) { throw new Error(\"Missing Environment variable: APPINSIGHTS_INSTRUMENTATIONKEY\")} // Application Insights will automatically recognize the instrumentation key localized in the environment variablesappInsights.setup() .setAutoDependencyCorrelation(true) .setAutoCollectRequests(true) .setAutoCollectConsole(true, true); appInsights.defaultClient.commonProperties = { environment: NODE_ENV,}; // No need to submit logs in development environmentappInsights.defaultClient.config.disableAppInsights = NODE_ENV === 'development'; export default appInsights; Copy In your index.ts file, import the configured appInsights Client and start the service. Place this command right beneath the other import statements. import appInsights from './middlewares/appInsights'; appInsights.start(); Copy More on how to configure Application insights can be found here: Monitor your Node.js services and apps with Application Insights","keywords":""},{"title":"API - Database","type":0,"sectionRef":"#","url":"express/API/Database","content":"","keywords":""},{"title":"Mongoose and MongoDB","type":1,"pageTitle":"API - Database","url":"express/API/Database#mongoose-and-mongodb","content":"Mongoose is an ORM (Object–relational mapping) for MongoDB that helps you with the more advanced interaction with MongoDB. Some of the features mongoose helps you with are: Population of foreign keysSimpler queryingStatic modellingData validation... much more! Included in this project is an docker-compose.yml file to run an local MongoDB database as an Docker Image, as well as an connection string in index.ts configured to connect to this database. This file is located here: assets/docker-compose.yml NOTE: This is meant for local development purposes only! Please move the connection string to .env and use an external MongoDB database or an Azure CosmosDB with MongoDB API "},{"title":"Other Databases","type":1,"pageTitle":"API - Database","url":"express/API/Database#other-databases","content":"For other SQL-like databases it is recommended to utilize an ORM (Object–relational mapping) framework. The most popular ones are: TypeORM - Main DocumentationSequelize - Main Documentation "},{"title":"Azure Cosmos DB (SQL API)","type":1,"pageTitle":"API - Database","url":"express/API/Database#azure-cosmos-db-sql-api","content":"@azure/cosmos - Azure Cosmos DB "},{"title":"API - Error Handling","type":0,"sectionRef":"#","url":"express/API/Error-Handling","content":"","keywords":""},{"title":"Error Handling","type":1,"pageTitle":"API - Error Handling","url":"express/API/Error-Handling#error-handling","content":"NOTE: This assumes that you have an implementation of an Express server in your project Most of what we do in an API is asynchronous and express does not support asynchronous error handling out of the box. We will use an external dependency for this instead of handling this ourself. express-async-errors - ES6 async/await support hack for ExpressJS npm i express-async-errors Next up is importing this package. On the top of your index.ts file add this import statement above the app-import. import 'express-async-errors';import app from './config/app'; Copy Next up is creating the error handler middleware. Create an new folder called middlewares inside the src folder. and inside here a new file called errorHandler.ts import { Request, Response, NextFunction } from \"express\" const errorHandler = (err: Error, req: Request, res: Response, next: NextFunction) => { return res.status(500).json({ message: err.message })} export default errorHandler; Copy Next go to the file where you initialize the express server and add the following code snippet: // Where you initialize the express server// const app = express(); // where you registerer the routes // It is important that this is below the point where you registerer the routes. app.use(errorHandler) Copy Now you will be able to throw Node.js Errors wherever you want in your codebase. To read more about this visit Express' API Documentation here: Express Error Handling "},{"title":"Security - Authentication","type":0,"sectionRef":"#","url":"express/API/Security/Authentication","content":"Security - Authentication NOTE: This assumes that you have an Azure AD application registration created for your project In this example i'll configure an single tenant application For authentication well configure Passport to check and validate the token provided by the client. Create a new file called passport.ts inside the middlewares folder. import passport from 'passport';import { BearerStrategy, ITokenPayload, VerifyCallback } from 'passport-azure-ad'; const { APP_CLIENT_ID, INTILITY_TENANT_ID } = process.env; /** * Create a new passport BearerStrategy to handle authenticate and decoding of the provided token. * Complete setup documentation can be found here https://github.com/AzureAD/passport-azure-ad#42-bearerstrategy. */ const mainStrategy = new BearerStrategy( { identityMetadata: `https://login.microsoftonline.com/${INTILITY_TENANT_ID}/v2.0/.well-known/openid-configuration`, clientID: `${APP_CLIENT_ID}`, audience: [ `${APP_CLIENT_ID}`, `api://${APP_CLIENT_ID}` ], loggingLevel: 'warn', // Validate the `iss` claim in id_token against user provided issuer validateIssuer: true, // List of allowed issuers issuer: [ `https://sts.windows.net/${INTILITY_TENANT_ID}/` ] }, (token: ITokenPayload, done: VerifyCallback) => { if (!token.oid) { done(new Error('User OID is not found in token')); } else { done(null, token); } },); // Tell passport to use the configured strategypassport.use('mainStrategy', mainStrategy); /** * Use Passport to authenticate the token provided. * It is important that the label defined in passport.use function match the label provided here */export const authenticate = passport.authenticate([ 'mainStrategy' ], { session: false }); // export configures passport instanceexport default passport; Copy Now its time to configure Express to use the passport middleware. In your Express config file (config/app.ts) add the following code snippet // const app = express(); // activating auth on serverapp.use(passport.initialize());app.use(passport.session()); Copy Next and at last, secure your endpoints or express router by adding the middleware. e.g. router.get(\"/\", authenticate, controller.getByOid);router.post(\"/\", authenticate, controller.createNewUser); Copy","keywords":""},{"title":"API - Express","type":0,"sectionRef":"#","url":"express/API/Express","content":"API - Express Express is a web framework for Node that allows us to create a HTTP server where we can create our API. express - Express web framework for Node.js. npm install express @types/express - Typings for Express.js. npm install -D @types/express Getting started with an Express app is fairly easy and can be done by a couple of lines of code. This is great if you want to test something, but larger enterprise applications require a more robust structural layered design. You can find the easy to follow getting started guide at Express' homepage: \"Express Hello World\". But in larger enterprise applications a more structured system is needed. In this repo i'll follow a multi-layered API architecture which divides the API into 3 layers: Routes and Controllers - This is where you handle the incoming requestsServices - Where you execute business logic on the entity, e.g. fetching other entities, calculating stuff.Repositories and Providers - Where you execute requests and/or queries to the database. NOTE: In this architecture it is important that the communication between each layer always flows downwards or sideways, never upwards. Like a service calling an controller. Project folder structureThis repository contains an example of an multilayered API design structure, as well as an example API for an User entity.📂 src┣ 📂 routes┃ ┗ 📜 baseRouter.ts // This is where you will register all your routes, this router is exposed at `/api/v1`.┣ 📂 controllers // This is where you handle the incoming requests registered by the routers, only simple data validation is done at this level.┣ 📂 services // This is where you handle the more advanced business logic, like like logic that span across to other database entities.┣ 📂 repository // This is where you write the database CRUD-operations like database queries etc.┣ 📂 models // This is where you define the Mongoose Schemas for validation.┣ 📂 middlewares // This is where you store all the custom express middlewares┣ 📂 interfaces // This is where you define all typescript interfaces, types, enums etc.┣ 📂 utils // Other utility functions.┣ 📂 config ┃ ┣ 📜 appInsights.ts // Configuration of the App Insights framework.┃ ┣ 📜 mongoose.ts // Configuration of the Mongoose framework.┃ ┗ 📜 app.ts // Configuration the Express server. Copy To get you started this template provides a User entity with CRUD routes, controller, service and a repository for this entity.","keywords":""},{"title":"Security - CORS","type":0,"sectionRef":"#","url":"express/API/Security/CORS","content":"Security - CORS Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served. By default only services (clients and servers) hosted on the same server can access each other. To allow other services to access your server you have to whitelist them using HTTP Headers manually or by using dependencies that helps you out with this. Go ahead and install this dependency: cors - Allows for configuring cors without touching the header values manually. npm i cors @types/cors - Typescript typing definitions for cors package. npm i -D @types/cors Next go to the file where you initialize the express server and add the following code snippet: import cors from \"cors\" // Where you initialize the express server// const app = express(); app.use(cors()); Copy This allow all external services to connect to your server, which is not ideal. The best solution is to whitelist domains by configuring the cors middleware. app.use(cors({ origin: [ 'my-super-awesome-frontend.azurewebsites.net', 'my-cool-background-process.azurewebsites.net' ] })); Copy There are many ways to configure cors, check out the npm packages documentation for more info.","keywords":""},{"title":"Security - Authorization","type":0,"sectionRef":"#","url":"express/API/Security/Authorization","content":"","keywords":""},{"title":"Roles","type":1,"pageTitle":"Security - Authorization","url":"express/API/Security/Authorization#roles","content":"An official guide on how to configure your application registration to use app roles and receiving them in the token can be found in this guide: How to: Add app roles to your application and receive them in the token Validating and checking the token is done by creating an Express middleware that validating the claims provided by the token. Passport saves the decoded token fields in the request object. Add this function to your middlewares/passport.ts file: /** * Check and validate the decoded token. * * @param {string[]} acceptedRoles */ export const authorize = (acceptedRoles: string[]) => (req: Request, res: Response, next: NextFunction): Response | void => { const decodedUserInfo = req.user; if (decodedUserInfo) { const { roles } = decodedUserInfo; // Validate the provided whitelisted roles against token checkRoles(roles, acceptedRoles) .then(() => next()) .catch(() => { return res.status(403).json({ message: 'You are unauthorized to access this resource.' }); }); } else { return res.status(401).json({ message: 'Unable to locate user info from decoded token' }); }}; /** * Check if the users roles is defined and if users roles is in acceptedRoles * * @param {string[]} userRoles * @param {string[]} acceptedRoles */ const checkRoles = (userRoles: string[], acceptedRoles: string[]): Promise<unknown> => { return new Promise((resolve, reject) => { if (!userRoles) { reject(false); } return userRoles.some((r) => acceptedRoles.includes(r)) ? resolve(true) : reject(false); });}; Copy Next, secure your endpoints or express router by adding the middleware. e.g. router.get(\"/\", authenticate, authorize(['<ROLE_NAME>']), controller.getByOid);router.post(\"/\", authenticate, authorize(['<ROLE_NAME>']), controller.createNewUser); Copy "},{"title":"Account type and Tenant ID","type":1,"pageTitle":"Security - Authorization","url":"express/API/Security/Authorization#account-type-and-tenant-id","content":"tid or tenant ID is an claim provided natively by azure AD and needs no configuration. To check if the user is an guest user in your tenant you need to add the acct optional claim to your token. To do this, follow this guide Provide optional claims to your app. Alter the if-statement in the authorize() configured in the previous step. if (decodedUserInfo) { const { roles, acct, tid } = decodedUserInfo; // Option 1: // Check authenticated users TenantID and block users outside of the AA-Intility tenant if (tid !== INTILITY_TENANT_ID) { return res.status(401).json({ message: 'Users outside of provided tenant is not permitted' }); } // Option 2: // Check authenticated users account status. Requires \"acct\" as an optional claim in the App Registration. // Regular user = 0, Guest user = 1. if (acct !== 0) { return res.status(401).json({ message: 'Guest users is not allowed' }); } // REST OF ROLE VALIDATION } else { return res.status(401).json({ message: 'Unable to locate user info from decoded token' });} Copy "},{"title":"User values in token","type":1,"pageTitle":"Security - Authorization","url":"express/API/Security/Authorization#user-values-in-token","content":"It's equally important to ensure that the API only responds with data or allow actions which corresponds to the authorized user's role. An example can be that you've created a multi-tenant API with a common endpoint for fetching a company's mobile subscriptions. It's then essential to actually force that multi-tenancy in your code. To accomplish this we can take a look at some of the claims provided in the token. We use token claims because they contain information about the user which are provided by a trusted resource (AD/Azure AD). upn, User principal name: this is the same as primary email for Intility users and can be used to identify the user.tid, Tenant ID: this can either be used alone if you have this filed in your dataset, or with ROT-OData API to lookup the company GUID or company code. This is especially important for multi tenant applications. "},{"title":"BONUS: ROT OData API query","type":1,"pageTitle":"Security - Authorization","url":"express/API/Security/Authorization#bonus-rot-odata-api-query","content":"The following query can be used to get a Company's GUID and/or code (e.g. AA) by using the ROT OData API. You can find the Official API documentation here: open-rot-api <ROT_BASE_URL>/ROTDirectoryAzureTenant?$filter=AzureTenantGUID eq <TENANT_ID_FROM_TOKEN>&$select=Directory, AzureTenantGUID&$expand=Directory($select=Name, CompanyGUID) "},{"title":"Security - HTTP","type":0,"sectionRef":"#","url":"express/API/Security/HTTP","content":"","keywords":""},{"title":"Security Headers","type":1,"pageTitle":"Security - HTTP","url":"express/API/Security/HTTP#security-headers","content":"Helmet is a tool to help you secure your Express app by applying various HTTP headers aimed at securing your API against known HTTP vulnerabilities. helmet - Helmet main package npm install helmet In your app.ts import helmet from \"helmet\"; //const app = express(); app.use(helmet()); // rest of config Copy To read more about helmet, please have a look at the documentation located at the README of the project. "},{"title":"HTTP Parameter Pollution (HPP)","type":1,"pageTitle":"Security - HTTP","url":"express/API/Security/HTTP#http-parameter-pollution-hpp","content":"HPP is a tool to help you secure your Express app by protecting against HTTP Parameter Pollution attacks. HTTP Parameter Pollution, or HPP, is a vulnerability that occurs due to the passing of multiple parameters having the same name. There is no RFC standard on what should be done when it has passed multiple parameters. This vulnerability was first discovered in 2009. HPP could be used for cross channel pollution, bypassing CSRF protection and WAF input validation checks. hpp - HPP main package npm install hpp In your app.ts import hpp from \"hpp\"; //const app = express();app.use(bodyParser.urlencoded()); // Make sure the body is parsed beforehand.app.use(hpp()); // rest of config Copy To read more about helmet, please have a look at the documentation located at the README of the project. "},{"title":"HTTP Content Length validation","type":1,"pageTitle":"Security - HTTP","url":"express/API/Security/HTTP#http-content-length-validation","content":"To not have your application vulnerable to large payload attacks you can restrict the maximum allowed payload size for incoming requests. express-content-length-validator - Express Content Length Validator main package npm install express-content-length-validator In your app.ts import contentLength from 'express-content-length-validator'; //const app = express();const MAX_CONTENT_LENGTH_ACCEPTED = 1048576; // 1MB app.use(contentLength.validateMax({max: MAX_CONTENT_LENGTH_ACCEPTED, status: 413, message: \"Payload Too Large\"})); // rest of config Copy "},{"title":"TypeScript - Development Environment","type":0,"sectionRef":"#","url":"express/TypeScript/development-environment","content":"","keywords":""},{"title":"Configuring Nodemon","type":1,"pageTitle":"TypeScript - Development Environment","url":"express/TypeScript/development-environment#configuring-nodemon","content":"Nodemon is a tool used in your development environment to watch for changes in your project and hot reload the server when files are updated on save. To configure nodemon you can create a nodemon.json file at the project root level. { \"ignore\": [ \"**/*.test.ts\", \"**/*.spec.ts\", \".git\", \"node_modules\" ], \"watch\": [ \"src\" ], // Script to be executed on nodemon start. \"exec\": \"node --inspect=5858 -r ts-node/register ./src/index.ts\", \"ext\": \"ts\"} Copy "},{"title":"Configuring VS Code debugger","type":1,"pageTitle":"TypeScript - Development Environment","url":"express/TypeScript/development-environment#configuring-vs-code-debugger","content":"Create a new file in .vscode/ directory at root level called launch.json. { \"version\": \"0.2.0\", \"configurations\": [ // Other debug configs { \"type\": \"node\", \"request\": \"launch\", \"name\": \"Launch Program\", \"runtimeArgs\": [ \"-r\", \"ts-node/register\" ], \"args\": [ \"${workspaceFolder}/src/index.ts\" ] } ]} Copy After creating this file you can execute the start:dev script you created in the previous step. You can read more about configuring the VS Code debugger in this official documentation provided by VS Code: Node.js debugging in VS Code "},{"title":"TypeScript - Environment Variables","type":0,"sectionRef":"#","url":"express/TypeScript/environment-variables","content":"TypeScript - Environment Variables The most usual way to store your secrets is by using environment variables. In production these is best applied to the host OS running the server. But in development mode we can use an dependency to load environment variables from a .env file. dotenv: Loads environment variables from a .env file into process.env npm i dotenv Next create a file named .env in the root level of your project. The values stored in this file need to follow a given syntax <KEY>=<VALUE>. # Set the node environment to be development. Use production instead in your production environment.NODE_ENV=development # Here you can add your own secrets Copy Then at the very top of the index.ts file, before all import statements insert the following code snippet. import dotenv from \"dotenv\"dotenv.config() Copy Now you can access environment variables by using the process.env.<KEY> value in Node.","keywords":""},{"title":"TypeScript - Starting a new project","type":0,"sectionRef":"#","url":"express/TypeScript/Starting-a-new-project","content":"","keywords":""},{"title":"Configuring TypeScript","type":1,"pageTitle":"TypeScript - Starting a new project","url":"express/TypeScript/Starting-a-new-project#configuring-typescript","content":"By creating the project with npx tsc --init you get a default set of compile rules. This tool documents many of the most used configuration options in the tsconfig.json file. These are in general great, but we do need to do some minor customizations. First of all you need to apply these changes to the properties in the tsconfig.json file. { \"compilerOptions\" : { // Rest of config \"outDir\": \"build\", \"sourceMap\": true, \"moduleResolution\": \"node\" }, \"exclude\": [ \"node_modules\" ]} Copy "},{"title":"Build tools","type":1,"pageTitle":"TypeScript - Starting a new project","url":"express/TypeScript/Starting-a-new-project#build-tools","content":"To get started you need these packages in your production environment: rimraf: Cross OS CLI-tool to delete files/directories. npm i rimraf Also install these dependencies needed for development purposes: typescript - Install TypeScript compilers@types/node - Type definitions for Node.js projects.nodemon - Development tool for automatically restarting Node.js projects on file save.npm-run-all - CLI tool to run multiple npm-scripts in parallel or sequential.ts-node - Running TypeScript without transpiring npm i -D typescript @types/node nodemon npm-run-all ts-node "},{"title":"Compiling and running the project","type":1,"pageTitle":"TypeScript - Starting a new project","url":"express/TypeScript/Starting-a-new-project#compiling-and-running-the-project","content":"To get started you can create a new directory called src at the project root level (where the package.json file is). Then create a new file called index.ts inside here. src/: This is the main code directory of your project. All code that should be compiled should be inside here.src/index.ts: This is the root file of your project. This is file that is executed when you start the project. To run your project you need to add these following scripts to the package.json file. { \"scripts\": { \"clean\": \"rimraf build\", \"compile\": \"tsc\", \"start\": \"node build/index.js\", \"start:dev\": \"nodemon\", \"start:prod\": \"run-s clean compile start\", \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }} Copy To run these scripts you can run the command npm run <script_name> in the terminal. At this point you can start the project with the command npm run start:prod. "},{"title":"TypeScript - ESLint","type":0,"sectionRef":"#","url":"express/TypeScript/ESLint","content":"TypeScript - ESLint ESLint is a tool for automatically finding and fixing formatting problems in your JavaScript code, this tool can be configured by VS Code to run on all file save. so you don't need to trigger this yourself. This helps you maintain a certain code style without needing to apply this yourself. ESLint - Main ESLint framework.@typescript-eslint/eslint-plugin - An ESLint plugin which provides lint rules for TypeScript codebases.@typescript-eslint/parser - Allows ESLint to lint TypeScript source code. npm i -D eslint @typescript-eslint/eslint-plugin @typescript-eslint/parser Create a new .eslintrc.json file to configure the ESLint framework. { \"parser\": \"@typescript-eslint/parser\", \"parserOptions\": { \"ecmaVersion\": 2020, \"sourceType\": \"module\" }, \"extends\": [ \"eslint:recommended\", \"plugin:@typescript-eslint/recommended\" ], \"rules\": { // List of available rules: https://eslint.org/docs/rules/ \"no-console\": [\"error\", {\"allow\": [\"warn\", \"error\", \"info\", \"table\"]}], \"quotes\": [\"warn\", \"single\", { \"allowTemplateLiterals\": true }], \"semi\": [\"error\", \"always\"], \"eol-last\": \"error\", \"object-curly-spacing\": [\"warn\", \"always\"], \"array-bracket-spacing\": [\"warn\", \"always\"], \"@typescript-eslint/ban-ts-comment\": \"off\" }} Copy Add two new scripts to package.json: { \"scripts\": { \"lint\": \"eslint src/**/*.ts\", \"lint:fix\": \"eslint src/**/*.ts --fix\" }} Copy Open user settings for VS Code by entering Ctrl + Shift + P and search for Open Settings. If you want to alter the settings for all projects using eslint select Open Settings (JSON). If you only want to alter the settings for this project select Open Workspace Settings (JSON). Add the following settings to this file: { // Rest of config \"editor.codeActionsOnSave\": { \"source.fixAll.eslint\": true, \"eslint.autoFixOnSave\": true }, \"eslint.codeAction.showDocumentation\": { \"enable\": true }, \"eslint.format.enable\": true, \"eslint.alwaysShowStatus\": true,} Copy","keywords":""}]